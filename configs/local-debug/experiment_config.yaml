# experiment config
exp_name: local-debug

train: True
resume: False


#pol_type: agent_pol #shared_pol shared policy or each agent will have its own policy
pol_type: shared_pol



algorithm:
  name: PPO
  # name: CentralizedCritic
  config: 'ppo_config.yaml'
environment_cls: 'FlexEnv'

#tune config
mode: min
metric: episode_reward_mean
# metric: /info/learner/pol_ag1/learner_stats/policy_loss

# HPO
# hpo_algo: 'asha' #'pbt'
hpo_algo: 'grid_search'
asha_params:
  n_samples: 1
  time_attribute: 'training_iteration'
  max_time: 5
  grace: 2
  red_factor: 3


# Run config
verbose: 0

#trainable config
n_iters: 2
checkpoint_freq: 1

#resources
num_cpu_head: 2.0
num_gpu_head: 0.0

num_cpu_node: 2.0
num_gpu_node: 0.0

n_factor: 3


evaluation:
  evaluation_interval: 1
  evaluation_duration: 5
  evaluation_duration_unit: 'episodes'
  evaluation_sample_timeout_s: 180.0
  evaluation_parallel_to_training: False



spill_dir: /home/omega/Documents/FCUL/Projects/FlexEnvMas/raylog/spill1
